{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYzTszXJ6JGnWFi/Cn/nCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4rt10m/scrapers/blob/main/TP_Scrape_Multi_site_Multi_term.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1bghCp2Dsbd",
        "outputId": "4762987b-aa07-49a5-c0b5-7ac24a254e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Processing search terms for www.worldremit.com:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:03<00:12,  3.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:06<00:09,  3.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:08<00:05,  2.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:11<00:02,  2.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:14<00:00,  2.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com:  11%|█         | 1/9 [00:14<01:53, 14.20s/it]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:02<00:11,  2.93s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:05<00:08,  2.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:08<00:05,  2.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:11<00:02,  2.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:14<00:00,  2.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com:  22%|██▏       | 2/9 [00:28<01:39, 14.16s/it]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:02<00:10,  2.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:05<00:07,  2.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:08<00:05,  2.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:11<00:02,  2.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:13<00:00,  2.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com:  33%|███▎      | 3/9 [00:42<01:24, 14.08s/it]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:02<00:10,  2.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:04<00:07,  2.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:07<00:05,  2.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:10<00:02,  2.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:12<00:00,  2.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com:  44%|████▍     | 4/9 [00:55<01:07, 13.55s/it]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:03<00:13,  3.39s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:06<00:08,  2.99s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:09<00:05,  2.97s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:11<00:02,  2.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:14<00:00,  2.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com:  56%|█████▌    | 5/9 [01:09<00:55, 13.94s/it]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:02<00:10,  2.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:04<00:07,  2.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:06<00:04,  2.24s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:08<00:02,  2.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:10<00:00,  2.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com:  67%|██████▋   | 6/9 [01:20<00:38, 12.93s/it]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:02<00:08,  2.07s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:04<00:06,  2.04s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:06<00:04,  2.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:08<00:02,  2.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:10<00:00,  2.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com:  78%|███████▊  | 7/9 [01:31<00:24, 12.20s/it]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:02<00:11,  2.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:05<00:08,  2.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:08<00:05,  2.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:10<00:02,  2.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:13<00:00,  2.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com:  89%|████████▉ | 8/9 [01:44<00:12, 12.62s/it]\u001b[A\n",
            "\n",
            "Processing pages:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  20%|██        | 1/5 [00:03<00:12,  3.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  40%|████      | 2/5 [00:05<00:08,  2.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  60%|██████    | 3/5 [00:08<00:05,  2.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages:  80%|████████  | 4/5 [00:11<00:02,  2.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Processing pages: 100%|██████████| 5/5 [00:14<00:00,  3.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "                                                               \u001b[A\u001b[A\n",
            "Processing search terms for www.worldremit.com: 100%|██████████| 9/9 [01:59<00:00, 13.27s/it]\u001b[A\n",
            "Processing URLs: 100%|██████████| 1/1 [01:59<00:00, 119.59s/it]\n"
          ]
        }
      ],
      "source": [
        "from time import sleep\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def soup2list(src, list_, attr=None):\n",
        "    if attr:\n",
        "        for val in src:\n",
        "            list_.append(val[attr])\n",
        "    else:\n",
        "        for val in src:\n",
        "            list_.append(val.get_text())\n",
        "\n",
        "# Define the URLs and search terms\n",
        "urls = ['www.worldremit.com']  # Add more URLs as needed\n",
        "search_terms = ['international', 'fees,cost,fee,costs', 'exchange rate,exchange rates','speed','limit','feature,features','safe,security,safety,secure','ease,easy','support,customer support']  # Add more search terms as needed\n",
        "\n",
        "from_page = 2\n",
        "to_page = 6\n",
        "\n",
        "# Iterate over each URL\n",
        "for company in tqdm(urls, desc=\"Processing URLs\"):\n",
        "    # Iterate over each search term for the current URL\n",
        "    for search in tqdm(search_terms, desc=f\"Processing search terms for {company}\", leave=False):\n",
        "        users = []\n",
        "        ratings = []\n",
        "        locations = []\n",
        "        dates = []\n",
        "        reviews = []\n",
        "\n",
        "        # Scrape the specified range of pages\n",
        "        for i in tqdm(range(from_page, to_page + 1), desc=\"Processing pages\", leave=False):\n",
        "            result = requests.get(fr\"https://www.trustpilot.com/review/{company}?page={i}&search={search}\")\n",
        "            soup = BeautifulSoup(result.content, features=\"lxml\")\n",
        "\n",
        "            # Temporary lists to hold the scraped data for the current page\n",
        "            temp_users = []\n",
        "            temp_locations = []\n",
        "            temp_dates = []\n",
        "            temp_ratings = []\n",
        "            temp_reviews = []\n",
        "\n",
        "            soup2list(soup.find_all('span', {'class': 'typography_heading-xxs__QKBS8 typography_appearance-default__AAY17'}), temp_users)\n",
        "            soup2list(soup.find_all('div', {'class': 'typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_detailsIcon__Fo_ua'}), temp_locations)\n",
        "            soup2list(soup.find_all('div', {'class': 'styles_reviewHeader__iU9Px'}), temp_dates)\n",
        "            soup2list(soup.find_all('div', {'class': 'styles_reviewHeader__iU9Px'}), temp_ratings, attr='data-service-review-rating')\n",
        "            soup2list(soup.find_all('div', {'class': 'styles_reviewContent__0Q2Tg'}), temp_reviews)\n",
        "\n",
        "            # Ensure all temporary lists are of the same length\n",
        "            max_length = max(len(temp_users), len(temp_locations), len(temp_dates), len(temp_ratings), len(temp_reviews))\n",
        "\n",
        "            temp_users.extend([\"BLANK\"] * (max_length - len(temp_users)))\n",
        "            temp_locations.extend([\"BLANK\"] * (max_length - len(temp_locations)))\n",
        "            temp_dates.extend([\"BLANK\"] * (max_length - len(temp_dates)))\n",
        "            temp_ratings.extend([\"BLANK\"] * (max_length - len(temp_ratings)))\n",
        "            temp_reviews.extend([\"BLANK\"] * (max_length - len(temp_reviews)))\n",
        "\n",
        "            users.extend(temp_users)\n",
        "            locations.extend(temp_locations)\n",
        "            dates.extend(temp_dates)\n",
        "            ratings.extend(temp_ratings)\n",
        "            reviews.extend(temp_reviews)\n",
        "\n",
        "            # To avoid throttling\n",
        "            sleep(1)\n",
        "\n",
        "        # Add columns for the URL and search term\n",
        "        urls_column = [company] * len(users)\n",
        "        search_terms_column = [search] * len(users)\n",
        "\n",
        "        # Convert the data to a DataFrame\n",
        "        review_data = pd.DataFrame(\n",
        "            {\n",
        "                'URL': urls_column,\n",
        "                'Search Term': search_terms_column,\n",
        "                'Username': users,\n",
        "                'Location': locations,\n",
        "                'Date': dates,\n",
        "                'Content': reviews,\n",
        "                'Rating': ratings\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Create a filename for the current URL and search term\n",
        "        filename = f\"REVIEW - {company.replace('www.', '').replace('.com', '')} - {search}.csv\"\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        review_data.to_csv(filename, index=False)"
      ]
    }
  ]
}